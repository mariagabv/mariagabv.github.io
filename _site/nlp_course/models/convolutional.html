<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Convolutional Models for Text</title>
    <meta name="description" content="Convolutional Neural Networks from building blocks to applications for specific tasks.">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/nlp_course/models/convolutional.html">

    <!-- diff from head.html begin -->
     <link rel="mask-icon" href="../resources/lectures/ico/plaza_mayor.png">
     <link rel="alternate icon" class="js-site-favicon" type="image/png" href="../resources/lectures/ico/plaza_mayor.png">
     <link rel="icon" class="js-site-favicon" type="image/svg+xml" href="../resources/lectures/ico/plaza_mayor.png">
    <!-- diff from head.html end -->

    <!--<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>-->



</head>


  <!--style>
  p {
    text-align: justify;
  }
  </style-->

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">María Velasco</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/posts.html">Blog</a>
          
        
          
          <a class="page-link" href="/cv.html">CV</a>
          
        
          
          <a class="page-link" href="/papers.html">Publications</a>
          
        
          
          <a class="page-link" href="/nlp_course.html">RAG en <strong><font color="#C43116">español</font></strong></a>
          
        
<!--        <a href="">
            <img src="image/UoS.ico" alt="DCS">
        </a>-->
      </div>
    </nav>

  </div>

</header>



    <!-- the next two lines are inserted once per page even if there are several shtukovinas,
     the rest is one-per-shtukovina; read more: https://flickity.metafizzy.co/options.html -->
<link rel="stylesheet" href="https://unpkg.com/flickity@2/dist/flickity.min.css" media="screen">
<script src="https://unpkg.com/flickity@2/dist/flickity.pkgd.min.js"></script>

    <!-- MathJax -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>



.demo_sidebar {
  margin: 0;
  padding: 0;
  background-color: white;

  position: relative;
    height: auto;
    width: 290px;
}

.demo_sidebar_text {
    font-size: 18px;
}

.demo_sidebar_comment {
    font-size:15px;
    margin-left:10px;
    font-family: "Comic Neue", "Arial";

}



.demo_sidebar a {
  display: block;
  color: black;
  padding: 8px;
  text-decoration: none;

}

.demo_sidebar a li {
  margin-left: 5px;
  padding: 0px;
}



.demo_sidebar a:hover {
  box-shadow: 0 3px 6px #6e8f27, 0 1px 1px #6e8f27;
}

.demo_sidebar a:hover:not(.active) {
  background-color: #f3f3f3;
}

.demo_sidebar .main_components {
  background-color: #f3f3f3;
}
.demo_sidebar .extra_components {
  background-color: #eaeaea;
}


#demo_sidebar_research_thinking:hover {
  box-shadow: 0 3px 6px #a68305, 0 1px 1px #a68305;

}


#demo_sidebar_related_papers:hover {
  box-shadow: 0 3px 6px #8a4972, 0 1px 1px #8a4972;
}


#demo_sidebar_fun:hover {
  box-shadow: 0 3px 6px #377b94, 0 1px 1px #377b94;
}

</style>



<style>

@import url('https://fonts.googleapis.com/css?family=Josefin+Sans&display=swap');
@import url("https://fonts.googleapis.com/css2?family=Patrick+Hand&display=swap");
@import url("https://fonts.googleapis.com/css2?family=Comic+Neue&display=swap");



#main_page_content {
  margin-left: 300px;
  padding: 30px;
  padding-left: 70px;
  text-align: justify;
}

.sidebar {
  margin: 0;
  padding: 0;
  width: 270px;
  background-color: #fafafa;
  position: fixed;
  height: 100%;
  overflow: auto;
  z-index: 1;
}

.sidebar a, .dropdown-btn {
  display: block;
  color: black;
  padding: 8px;
  text-decoration: none;
  border-right: 5px solid #C43116;
}


.sidebar a li {
  margin-left: 10px;
  padding: 0px;
}

.dropdown-container {
  display: none;
  background-color: #f4f4f4;
}

.active_caret .fa-caret-down
{
    color: #C43116;
    font-size: 30px;
}

.fa-caret-down {
  float: right;
  padding-right: 8px;
}

.sidebar a.active {
  background-color: #e3e3e3;
  color: black;
}

.sidebar a:hover {
  box-shadow: 0 3px 6px #D7472C, 0 1px 1px #D7472C;
}

.sidebar a:hover:not(.active) {
  background-color: #f3f3f3;
}

.sidebar .extra_components {
  background-color: #eaeaea;
}

#sidebar_analysis {
    padding: 10px;
}

#sidebar_research_thinking {
padding: 10px;
  border-right: 5px solid #fad400;
}
#sidebar_research_thinking:hover {
  box-shadow: 0 3px 6px #a68305, 0 1px 1px #a68305;

}


#sidebar_related_papers {
padding: 10px;
  border-right: 5px solid #d192ba;
}
#sidebar_related_papers:hover {
  box-shadow: 0 3px 6px #8a4972, 0 1px 1px #8a4972;
}


#sidebar_fun {
padding: 10px;
  border-right: 5px solid #6fb7d1;
}
#sidebar_fun:hover {
  box-shadow: 0 3px 6px #377b94, 0 1px 1px #377b94;
}


.sidebar_ico {
    float: right;
    height: 20;
  }

div.content {
  margin-left: 200px;
  padding: 1px 16px;
  height: 1000px;
}

#sidebar_small { width: 60px; }


@media screen and (max-width: 1000px) {
  .sidebar {
    width: 200px;
  }
  #main_page_content {
  margin-left: 200px;
  padding-left: 50px;
}
#for_you_in_sidebar {display: none;}

  }


@media screen and (max-width: 900px) {

  div.content {margin-left: 0;}
  #main_page_content {margin-left: 80px; padding: 15px;}

  #demo_sidebar {}
}


.softmax_tau_bokeh {
    font-size: 16px;
  }



.card_with_ico {
    position: relative;
    padding: 10px;
    margin: 10px;
  }
  .card_with_ico p {
    padding: 10px;
  }
  .card_with_ico ul {
    padding: 10px;
  }
  .card_with_ico .text_box_green {
    border: 1px solid #d8e8b5;
    border-radius: 5px;
    margin-left: 30px;
  }
  .card_with_ico .text_box_pink {
    border: 1px solid #dec8d6;
    border-radius: 5px;
    margin-left: 30px;
  }
  .card_with_ico .text_box_yellow {
    border: 1px solid #f0e4a5;
    border-radius: 5px;
    margin-left: 30px;
  }
  .card_with_ico .ico {
    float: left;
    width: 25px;
  }



.text_box_green {
    border: 1px solid #C43116;
    border-radius: 5px;
    display: table;
    margin-left: 20px;
  }


.text_box_grey_background {
    border: 1px solid #C43116;  /* Increased border thickness to 2px */
    border-radius: 5px;
    display: table;
    margin-left: 20px;
    background-color: #f0f0f0;  /* Light grey background */
}


  .text_box_green p {
    padding: 10px;
    padding-bottom: 0px;
  }



.green_left_thought {
    border-left: 5px solid #C43116;
    margin: 10px;
    margin-left: 20px;
    padding: 0px;
    background-color: #F4F4F6;
  }
  .green_left_thought p {

    margin-left: 10px;
    padding: 5px;
  }


.box_green_left {
    border-left: 2px solid #C43116;
    margin-left: 10px;
    padding: 10px;
  }

.box_green_right {
    border-right: 2px solid #C43116;
    margin-right: 10px;
    padding: 10px;
  }

  .box_violet_right {
    border-right: 2px solid #67468f;
    margin-right: 10px;
    padding: 10px;
  }

.box_pink_left {
    border-left: 2px solid #7a3160;
    margin-left: 5px;
    padding: 10px;
  }

 .box_yellow_left {
    border-left: 2px solid #d6b000;
    margin-left: 5px;
    padding: 10px;
  }

  .box_blue_left {
    border-left: 2px solid #0278a1;
    margin-left: 5px;
    padding: 10px;
  }

.paper_title {
    text-align: center;
    padding: 5px;
    padding-top: 0px;
    font-size: 16px;

  }
  .paper_authors {
    font-size: 14px;
    text-align: center;
    margin-bottom: 10px;
  }
  p {
    text-align: justify;
  }


  .greenCard {
    width: 100%;
    border: 1px solid #ccc;
    border-radius: 1px;
    margin: 10px 5px;
    padding: 3px;
    display: grid;
    grid-template-rows: auto auto;
  }

  #thumbnail_green {
    box-shadow: 0 2px 4px #C43116, 0 1px 1px #C43116;
  }
  #thumbnail_green:hover {
    box-shadow: 0 6px 12px #751D0D, 0 4px 4px #751D0D;
  }

  #thumbnail_violet {
    box-shadow: 0 2px 4px #655578, 0 1px 1px #655578;
  }
  #thumbnail_violet:hover {
    box-shadow: 0 6px 12px #655578, 0 4px 4px #655578;
  }

  #thumbnail_paper {
    box-shadow: 0 2px 4px #ab859e, 0 1px 1px #ab859e;
  }
  #thumbnail_paper:hover {
    box-shadow: 0 6px 12px #ab859e, 0 4px 4px #ab859e;
  }



  #thumbnail_blue {
    box-shadow: 0 2px 4px #377b94, 0 1px 1px ;
  }
  #thumbnail_blue:hover {
    box-shadow: 0 6px 12px #377b94, 0 4px 4px #377b94;
  }

  .paperCard {
    width: 100%;
    border: 1px solid #ccc;
    border-radius: 1px;
    margin: 10px 5px;
    padding: 3px;
    display: grid;
    grid-template-rows: auto auto;
  }
  .paperIntro {
    display: grid;
    grid-template-columns: 70% 30%;
  }

  .showMePaper {
    box-shadow: 0 2px 4px #a68305, 0 2px 1px #a68305;
  }
  .showMePaper:hover {
    box-shadow: 0 3px 6px #ab859e, 0 4px 4px #ab859e;
  }


  .cardContent {
    padding: 10px;
  }

  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  .conf_name {
    float: right;
    font-family: sans-serif;
    font-size: 13px;
    margin-bottom: 0px;
    background-color: #f7edf4;
    border: 1px solid #d1a1c3;
    border-radius: 1px;
    padding: 0px 4px 0px 4px;
  }



  .paper_circle {
    height: 10px;
    width: 10px;
    background-color: #cf99be;
    border: 1px solid #8c2b6e;
    border-radius: 50%;
  }

  .research_circle {
    height: 10px;
    width: 10px;
    background-color: #ffda00;
    border: 1px solid #998300;
    border-radius: 50%;
  }

  .fun_circle {
    height: 10px;
    width: 10px;
    background-color: #68c7e8;
    border: 1px solid #0278a1;
    border-radius: 50%;
  }
  .green_circle {
    height: 10px;
    width: 10px;
    background-color: #DB8373;
    border: 1px solid #C43116;
    border-radius: 50%;
  }

.violet_circle {
    height: 10px;
    width: 10px;
    background-color: #b59fcf;
    border: 1px solid #67468f;
    border-radius: 50%;
  }

  .data_text {
    font-family: "Comic Neue", "Arial";
  }




   .researchCard {
    width: 100%;
    border: 1px solid #ccc;
    border-radius: 1px;
    margin: 10px 5px;
    padding: 3px;
    display: grid;
    grid-template-rows: auto auto;
  }
  .researchIntro {
    display: grid;
    grid-template-columns: 70% 30%;
  }
    #thumbnail_research {
    box-shadow: 0 2px 4px #a68305, 0 1px 1px #a68305;
  }
  #thumbnail_research:hover {
    box-shadow: 0 6px 12px #a68305, 0 4px 4px #a68305;
  }
  .research_title {
    text-align: center;
    padding: 5px;
    font-size: 18px;
    font-family: "Comic Neue", "Arial";
    background-color: #fafaf5;


  }
   .research_tag {
    float: right;
    font-family: sans-serif;
    font-size: 13px;
    margin-bottom: 0px;
    background-color: #f5f0d5;
    border: 1px solid #fad400;
    border-radius: 1px;
    padding: 0px 4px 0px 4px;
  }

    .research_question {
    font-weight:bold;
    font-size:18px;
    background-color:#F4F4F6;
    margin-right: 15px;
    padding-left: 5px;
    padding-right: 5px;
  }

  .research_summary {
    padding: 5px;
    font-size: 18px;
    font-family: "Comic Neue", "Arial";
    background-color: #F4F4F6;
    margin-left:10px;
  }


    .text_table td, .text_table th {
    text-align: center;
    padding-left: 10px;
    padding-right: 10px;
    padding-top: 2px;
    padding-bottom: 2px;
    border-bottom: 1px solid #dedeca; /* Add border for horizontal lines */
}


  </style>

<!--##################################################-->

    <div>
        <div class="sidebar" id="sidebar">
    <div class="sidebar_components">
<a href="javascript:void(0)" id="close_sidebar_btn" onclick="closeNav()"
   style="text-align:center;font-size:30px;padding:0px;">⇤</a>
   <a class="active" href="../../nlp_course.html" style="font-weight: bold;">
        <img height="18" class='sidebar_ico' src="../../resources/lectures/ico/course_logo.png" style="margin-right: 8px;margin-left: 8px;margin-top: 4px;"/>
        NLP Course <font color="#92bf32" id="for_you_in_sidebar">| For You</font></a>
        <a href="#main_content" style="font-weight: bold;">Convolutional Networks</a>
        <a href="#intro">Intuition</a>
        <a href="#typical_model">A Typical Model</a>


        <div class="dropdown-scope">
           <a class="dropdown-btn active_caret" >Building Blocks
            <i class="fa fa-caret-down"></i>
          </a>
          <div class="dropdown-container" style="display:block;">
              <a href="#blocks_convolution"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    Convolution</a>
              <a href="#blocks_pooling"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                  Pooling</a>
              <a href="#blocks_residual"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                  Residual Connections</a>
          </div>
        </div>


        <div class="dropdown-scope">
           <a class="dropdown-btn active_caret" >Models for Specific Tasks
            <i class="fa fa-caret-down"></i>
          </a>
          <div class="dropdown-container" style="display:block;">
              <a href="#text_classification"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                  Text Classification</a>
                <a href="#language_modeling"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    Language Modeling</a>
              <!--
              <a href="#machine_translation"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    Machine Translation</a>
                    -->
          </div>
        </div>


<a href="#analysis_interpretability" id="sidebar_analysis">Analysis and Interpretability
    <img height="25" src="../../resources/lectures/ico/analysis_empty.png" class="sidebar_ico"/></a>

</div>
</div>



<div class="sidebar" id="sidebar_small">

  <a class="active" onclick="openNav()" style="text-align:center;">☰</a>
    <a href="../nlp_course.html" style="font-weight: bold;">
        <img height="20" src="../../resources/lectures/ico/course_logo.png" style="margin-right: 8px;margin-left: 8px;"/></a>
    <a href="#main_page_content" style="text-align:center; font-size:20px;color:#7ca81e"> <i class="fa fa-home"></i></a>

<a href="#analysis_interpretability" id="sidebar_analysis"> <img height="25" src="../../resources/lectures/ico/analysis_empty.png"/></a>

</div>


<script>
function openNav() {
  document.getElementById("sidebar").style.display = "block";
  document.getElementById("sidebar_small").style.display = "none";
  document.getElementById("close_sidebar_btn").style.display = "block";
}

function closeNav() {
  document.getElementById("sidebar").style.display = "none";
  document.getElementById("sidebar_small").style.display = "block";
  document.getElementById("close_sidebar_btn").style.display = "none";
}

</script>


<script>
function onResize() {
  if (window.innerWidth >= 800) {
     document.getElementById("sidebar").style.display = "block";
     document.getElementById("sidebar_small").style.display = "none";
     document.getElementById("close_sidebar_btn").style.display = "none";
  }
  else {
     document.getElementById("sidebar").style.display = "none";
    document.getElementById("sidebar_small").style.display = "block";

  }
}
window.onresize = onResize;
onResize();
</script>

<script>
/* Loop through all dropdown buttons to toggle between hiding and showing its dropdown content - This allows the user to have multiple dropdowns without any conflict */
var dropdown = document.getElementsByClassName("dropdown-btn");
var i;

for (i = 0; i < dropdown.length; i++) {
  dropdown[i].addEventListener("click", function(event) {
  this.classList.toggle("active_caret");
  var dropdownButton = event.target || event.srcElement;
  while(dropdownButton.className != "dropdown-scope")
     dropdownButton = dropdownButton.parentElement;
  var dropdownContent = dropdownButton.getElementsByClassName("dropdown-container")[0];

  if (dropdownContent.style.display === "block") {
  dropdownContent.style.display = "none";
  } else {
  dropdownContent.style.display = "block";
  }
  });
}
</script>



<div class="wrapper" id="main_page_content">
    <div class="header"><h1>Convolutional Neural Networks for Text</h1></div>


<div class="main_content" id="main_content">

<div id="intro">
    <p class="data_text"><font color="#888">This is the <a href="">Convolutional Models Supplementary</a>.
            It contains a
            detailed description of convolutional models in general, as well as particular
            model configurations for specific tasks.<br><br>
        Most of the content is copied from the corresponding parts of the main course: I gathered them here
        for convenience.
        The news parts here are
        <a href="#parameters">Parameters: Kernel size, Stride, Padding, Bias</a> and
        <a href="#k_max_pooling">k-max pooling</a>.

    </font>
        </p>

    <h2>Convolutions for Images and Translation Invariance</h2>
    <p>Convolutional networks were originally developed for computer vision tasks. Therefore, let's first
    understand the intuition behind convolutional models for images.</p>
    <p>Imagine we want to classify an image into several classes, e.g. cat, dog, airplane, etc.
        In this case, if you find a cat on an image, you <font face="arial">don't care where</font> on the image
        this cat is: you care only that it is there somewhere.
    </p>

        <center>
        <img src="../../resources/lectures/models/cnn/translation_cats-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        </center>
    <p>
    </p>

    <div style="display:grid;grid-template-columns: 75% 25%;">

        <div>
            <p>Convolutional networks apply the same operation to small parts of an image: this is how they extract features.
                Each operation is looking for a match with a pattern,
                and a network learns which patterns are useful.
                With a lot of layers, the learned patterns become and more complicated: from lines in the early layers
                to very complicated patterns (e.g., the whole cat or dog) on the upper ones. You can look at the examples
                in the <a href="#analysis_interpretability">Analysis and Interpretability</a> section.
            </p>
            <p>This property is called <font face="arial">translation invariance</font>:
                <font face="arial">translation</font> because we are talking about shifts in space,
                <font face="arial">invariance</font> because we want it to not matter.
            </p>
        </div>
        <div>
            <p style="text-align: center; float: right; display: block; margin-left:25px;
                   margin-top:-10px; line-height:15px;">
                <video width="100%" height="auto" loop autoplay muted style="margin-left: 20px;">
                <source src="../../resources/lectures/models/cnn/cnn_with_cat.mp4" type="video/mp4">
                </video>
                <br />
                <span style="font-size: small;">The illustration is adapted from the one taken from
                <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">this cool repo</a>.</span>
            </p>
        </div>
    </div>



    <h2>Convolutions for Text</h2>

    <p>Well, for images it's all clear: e.g. we want to be able to move a cat because we don't care where the cat is.
    But what about texts? At first glance, this is not so straightforward: we can not move phrases easily -
        the meaning will change or we will get something that does not make much sense.
    </p>

    <p>However, there are some applications where we can think of the same intuition. Let's imagine
    that we want to classify texts, but not cats/dogs as in images, but positive/negative sentiment.
    Then there are some words and phrases which could be very informative "clues" (e.g.
        <font class="data_text"><strong>it's been great</strong></font>,
        <font class="data_text"><strong>bored to death</strong></font>,
        <font class="data_text"><strong>absolutely amazing</strong></font>,
        <font class="data_text"><strong>the best ever</strong></font>, etc), and others which are not important at all.
        We don't care much where in a text we saw
        <font class="data_text"><strong>bored to death</strong></font> to understand the sentiment, right?
    </p>

        <center>
        <img src="../../resources/lectures/models/cnn/translation_texts-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        </center>



<div id="typical_model">
    <h2>A Typical Model: Convolution+Pooling Blocks</h2>

    <p>Following the intuition above, we want to detect some patterns, but we don't
        care much where exactly these patterns are. This behavior is implemented with
        two layers:</p>
    <ul>
        <li><font face="arial">convolution</font>: finds matches with patterns (as the cat head we saw above);</li>
        <li><font face="arial">pooling</font>: aggregates these matches over positions (either locally or globally).</li>
    </ul>

    <p>A typical convolutional model for texts is shown on the figure.
        Usually,
        a convolutional layer is applied to word embedding,
        which is followed by a non-linearity
            (usually ReLU) and
            a pooling operation. These are the main building blocks of convolutional models:
        for specific tasks, the configurations can be different, but these blocks are standard.
    </p>



    <center>
    <img src="../../resources/lectures/models/cnn/typical_cnn_with_cat-min.png"
         style="max-width:100%; margin-bottom:20px;"/>
    </center>
    <p>In the following, we discuss in detail the main building blocks, convolution and pooling,
    then consider modeling modifications.
    </p>

    <p><u>Note</u> that modeling modifications for specific tasks are described in the corresponding lectures
    of the main part of the course. We repeat applications for specific tasks here just for convenience.</p>


</div>

</div>


    <div id="blocks_convolution">
    <h1>Building Blocks: Convolution</h1>



        <div style="display:grid;grid-template-columns: 75% 25%;">

            <div>
            <p>Convolutions in computer vision go over an image with a
                sliding window and apply the same operation,
                <font face="arial">convolution filter</font>, to each window.
                A convolution layer usually has several filters, and
                each filter detects a different pattern (more on this below).
            </p>

            <p>The illustration (taken from <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">this cool repo</a>)
            shows this process for one filter: the bottom is the input image, the top is the filter output.
                Since an image has two dimensions (width and height), the convolution is two-dimensional.
            </p>
            </div>
            <div>
                <p style="text-align: center; float: right; display: block; margin-left:25px;
         margin-top:-10px; line-height:15px;">
            <img src="../../resources/lectures/models/cnn/same_padding_no_strides.gif"/><br />
            <span style="font-size: small;">Convolution filter for images. The illustration is from
        <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">this cool repo</a>.</span></p>
            </div>
        </div>


        <div style="display:grid;grid-template-columns: 50% 50%;">
            <div>
                 <p>Differently from images, texts have only one dimension. Therefore, a convolution here is one-dimensional:
                 look at the illustration.
                 </p>

            </div>
            <div>
                <video width="90%" height="auto" loop autoplay muted style="margin-left: 20px;float:right;">
                <source src="../../resources/lectures/models/cnn/cnn_filter_reads_text.mp4" type="video/mp4">
                </video>
                <p style="font-size:14px;text-align:center;margin-top:-20px;">Convolution filter for text.</p>
            </p>

            </div>
        </div>


        <h2>Convolution is a Linear Operation Applied to Each Window</h2>
        <img src="../../resources/lectures/models/cnn/convolution_linear_layer-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        <p>A convolution is a linear layer (followed by a non-linearity) which is applied to each input window.
            Formally, let us assume that
        </p>
        <ul>
            <li>\((x_1, \dots, x_n)\) - representations of the input words, \(x_i\in \mathbb{R}^d\);</li>
            <li>\(d\) (<font face="arial">input channels</font>) - size of an input embedding;</li>
            <li>\(k\) (<font face="arial">kernel size</font>) - the length of a convolution window (on the illustration, \(k=3\));</li>
            <li>\(m\) (<font face="arial">output channels</font>) - number of convolution filters
                (i.e., number of channels produced by the convolution).</li>
        </ul>
        <p>Then a convolution is a linear layer \(W\in\mathbb{R}^{(k\cdot d)\times m}\).
         For a \(k\)-sized window \((x_i, \dots x_{i+k-1})\), the convolution
                    takes the concatenation of these vectors
                    \[u_i = [x_i, \dots x_{i+k-1}]\in\mathbb{R}^{k\cdot d}\]
                    and multiplies by the convolution matrix:
            \[F_i = u_i \times W.\]
            A convolution goes over an input
            with a sliding window and applies
            the same linear transformation to each window.
        </p>


        <h2 id="parameters">Parameters: Kernel size, Stride, Padding, Bias</h2>

        <h3><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Kernel size</font>: How far to look</h3>
        <p>Kernel size is the number of input elements (tokens) a convolution looks at each step.
        For text, typical values are 2-5.</p>

        <center>
        <img src="../../resources/lectures/models/cnn/kernel_size-min.png"
             style="max-width:80%; margin-bottom:20px;"/>
            </center>

        <h3><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Stride</font>: How much move a filter at each step</h3>
        <p>Stride tells how much to move filter at each step. For example, stride equal to 1 means that we
        move the filter by 1 input element (pixel for images, token for texts) at each step.</p>

        <center>
        <img src="../../resources/lectures/models/cnn/stride-min.png"
             style="max-width:80%; margin-bottom:20px;"/>
            </center>

        <h3><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Padding</font>: Add zero vectors to both sides</h3>

        <p>Padding adds zero vectors to both sides of an input. If you are using stride>1,
        you may need padding - be careful!</p>
        <center>
        <img src="../../resources/lectures/models/cnn/padding-min.png"
             style="max-width:80%; margin-bottom:20px;"/>
        </center>

        <h3><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Bias</font>: The bias term in the linear operation in convolution.</h3>
        <p>By default, there's no bias - only multiplication by a matrix.</p>

        <center>
        <img src="../../resources/lectures/models/cnn/bias-min.png"
             style="max-width:80%; margin-bottom:20px;"/>
        </center>


        <h2><font face="arial">Intuition:</font> Each Filter Extracts a Feature</h2>
        <p>Intuitively, each filter in a convolution extracts a feature.</p>
        <img src="../../resources/lectures/models/cnn/one_filter-min.png"
             style="max-width:50%; margin-left:20px; float:right;"/>
        <h3><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">One filter</font> - one feature extractor</h3>
                <p>A filter takes vector representations in a current window

                and transforms them linearly into a single feature. Formally,
                    for a window \(u_i = [x_i, \dots x_{i+k-1}]\in\mathbb{R}^{k\cdot d}\)
                    a filter
                    \(f\in\mathbb{R}^{k\cdot d}\) computes dot product:
                    \[F_i^{(f)} = (f, u_i).\]
                    The number \(F_i^{(f)}\) (the extracted "feature") is a result of applying the
                    filter \(f\) to the window \((x_i, \dots x_{i+k-1})\).
                </p>


        <h3><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">m filters</font>: m feature extractors</h3>

        <div style="display:grid;grid-template-columns: 45% 55%;">
            <div>
                <img src="../../resources/lectures/models/cnn/several_filters_read.gif"
             style="max-width:90%; margin-right:20px; margin-bottom: 20px; float:left;"/>

        <p>One filter extracts a single feature. Usually, we want many features: for this,
            we have to take several filters. Each filter reads an input text and extracts a different feature -
            look at the illustration.
            The number of filters is the number of output features you want to get. With \(m\) filters
                    instead of one, the size of
                the convolutional layer we discussed above will become \((k\cdot d)\times m\).
        </p>
            </div>
            <div>
                <img src="../../resources/lectures/models/cnn/several_filters-min.png"
             style="max-width:90%; margin-left:20px; float:right;"/>

            </div>
        </div>

        <p><font face="arial"><u>This is done in parallel!</u></font>
            Note that while I show you how a CNN "reads" a text, in practice these
            computations are done in parallel.
        </p>

    </div>

    <div id="blocks_pooling">
        <h1>Building Blocks: Pooling</h1>

        <p>After a convolution extracted \(m\) features from each window,
        a <font face="arial">pooling</font>
            layer summarises the features in some region.
            Pooling layers are used to reduce the input dimension, and, therefore, to reduce the number
            of parameters used by the network.</p>


        <h2> Max and Mean Pooling</h2>

        <p>
        The most popular is <font face="arial">max-pooling</font>: it takes maximum over each dimension, i.e. takes
            the maximum value of each feature.
        </p>
        <img src="../../resources/lectures/models/cnn/max_pooling-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        <p>Intuitively, each feature "fires" when it sees some pattern: a
            visual pattern in an image (line, texture, a cat's paw, etc) or
            a text pattern (e.g., a phrase).
            After a pooling operation,
        we have a vector saying which of these patterns occurred in the input.
        </p>
        <p><font face="arial">Mean-pooling</font> works similarly but computes mean over each
        feature instead of maximum.</p>

        <h2 id="k_max_pooling"> k-max Pooling</h2>

        <img src="../../resources/lectures/models/cnn/k_max_pooling-min.png"
             style="max-width:60%; margin-left:20px; float:right;"/>
        <p><font face="arial">k-max pooling</font> is a generalization of max-pooling.
        Instead of finding one maximum feature, it selects k features with the highest values. The order of these
            features is preserved.
        </p>

        <p>It can be useful if it is important how many times a network found some pattern.
        </p>

        <h2> Pooling and Global Pooling</h2>

        <p>Similarly to convolution, <font face="arial">pooling</font> is applied to windows
        of several elements. Pooling also has the stride parameter, and the most common approach is to use
        pooling with non-overlapping windows. For this, you have to set the stride parameter the same
        as the pool size. Look at the illustration.</p>

        <center>
        <img src="../../resources/lectures/models/cnn/pooling_with_stride-min.png"
             style="max-width:70%; margin-bottom:20px;"/>
            </center>

        <p>The difference between <font face="arial">pooling</font> and
        <font face="arial">global pooling</font> is that <font face="arial">pooling</font> is applied over features
        in each window independently, while
        <font face="arial">global pooling</font> performs over the whole input.
        For texts, global pooling is often used to get a single vector representing the whole text; such
        global pooling is called <font face="arial">max-over-time pooling</font>, where the "time" axis
        goes from the first input token to the last.</p>

        <center>
        <img src="../../resources/lectures/models/cnn/pooling_vs_global-min.png"
             style="max-width:90%; margin-bottom:20px;"/>
            </center>

        <p>Intuitively, each feature "fires" when it sees some pattern: a
            visual pattern in an image (line, texture, a cat's paw, etc) or
            a text pattern (e.g., a phrase).
            After a pooling operation,
        we have a vector saying which of these patterns occurred in the input.
        </p>

    <div class="card_with_ico">
    <img class="ico" src="../../resources/lectures/ico/analysis_empty.png" width="30px"/>
    <div class="text_box_green">
    <p class="data_text"> For more details
            on intuition and examples of patterns, look at the
            <a href="#analysis_interpretability">Analysis and Interpretability</a> section.
    </div>
    </div>

</div>


    <div id="blocks_residual">
        <h1>Building Blocks: Residual Connections</h1>
        <h3>TL;DR: Train Deep Networks Easily!</h3>

        <p>To process longer contexts you need a lot of layers. Unfortunately, when stacking a lot of layers,
            you can have a problem with propagating gradients
            from top to bottom
            through a deep network. To avoid this, we can use
            <font face="arial">residual connections</font> or a more complicated
            variant <font face="arial"><a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank">highway connections</a></font>.
        </p>
        <img src="../../resources/lectures/lang_models/neural/cnn/residual_highway-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        <p>Residual connections are very simple: they add input of a block to its output. In this way,
        the gradients over inputs will flow not only indirectly through the block, but also directly
        through the sum.
        </p>
        <p>Highway connections have the same motivation, but a use a gated sum of input and output instead of
        the simple sum. This is similar to LSTM gates where a network can learn the types of information
        it may want to carry on from bottom to top (or, in case of LSTMs, from left to right).
        </p>
        <div style="display:grid;grid-template-columns: 40% 60%;">
            <div>
                <p>Look at the example of a convolutional network with residual connections.
                    Typically, we put residual connections around blocks with several layers.
                    A network can several such blocks
                     - depending on your task, you may need a lot of layers to get a decent receptive field.
                </p>
            </div>
            <div><img src="../../resources/lectures/models/cnn/model_with_residual-min.png"
             style="max-width:90%; margin-left:20px;float:right;"/></div>

        </div>
    </div>

    <br>

    <div id="text_classification">
    <h1>Specific Tasks: Text Classification</h1>

        <p class="data_text"><font color="#888">This part is a summary
            of the convolutional models part of the <a href="../text_classification.html" target="_blank">Text Classification</a>
            lecture in the main part of the course. For a detailed description of the text classification task,
            go to the main lecture.
            </font>
        </p>

        <p>Now, when we understand how the convolution and pooling work, let's come to modeling modifications.
            In the case of text classification:</p>
        <div class="green_left_thought" style="font-size:18px;">
                <p class="data_text">
                    We need a model
                    that can produce a <strong>fixed-sized</strong> vector
                    for inputs of <strong>different</strong> lengths.
                </p>
            </div>

        <p>Therefore, we need to construct a convolutional model that represents a text as a single vector.</p>


        <p>
        The basic convolutional model for text classification is shown on the figure.
        Note that, after the convolution, we use <font face="arial">global-over-time pooling</font>.
            This is the key operation: it allows to compress a text into a single vector.
            The model itself can be different, but at some point, it has to use the global pooling to
            compress input in a single vector.
        </p>


        <center>
        <img src="../../resources/lectures/text_clf/neural/cnn/model_basic-min.png"
             style="max-width:80%; margin-bottom:20px;"/>
        </center>

        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Several Convolutions with Different Kernel Sizes</font></h4>

        <p>Instead of picking one kernel size for your convolution, you can use several convolutions
        with different kernel sizes. The recipe is simple: apply each convolution to the data,
        add non-linearity and global pooling after each of them,
            then concatenate the results (on the illustration, non-linearity is omitted for simplicity).
            This is how you get vector representation of the data which
            is used for classification.
        </p>

        <center>
        <img src="../../resources/lectures/text_clf/neural/cnn/several_kernel_sizes-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        </center>
        <p>This idea was used, among others, in the paper
        <a href="https://www.aclweb.org/anthology/D14-1181.pdf" target="_blank">
            Convolutional Neural Networks for Sentence Classification</a> and many follow-ups.
        </p>


        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Stack Several Blocks Convolution+Pooling</font></h4>


        <p>Instead of one layer, you can stack several blocks convolution+pooling on top of each other.
            After several blocks, you can apply another convolution, but with global pooling this time.
            Remember: you have to get a single fixed-sized vector - for this, you need global pooling.
        </p>
        <p>Such multi-layered convolutions can be useful when your texts are very long; for example,
        if your model is character-level (as opposed to word-level).</p>

        <center>
        <img src="../../resources/lectures/text_clf/neural/cnn/several_blocks-min.png"
             style="max-width:90%; margin-bottom:20px;"/>
        </center>
        <p>This idea was used, among others, in the paper
        <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
            Character-level Convolutional Networks for Text Classification</a>.
        </p>



    </div>


<div id="language_modeling">
    <h1>Specific Tasks: Language Modeling</h1>

    <p class="data_text"><font color="#888">This part is a summary
            of the convolutional models part of the <a href="../language_modeling.html" target="_blank">Language Modeling</a>
            lecture in the main part of the course. For a detailed description of the language modeling task,
            go to the main lecture.
            </font>
        </p>

        <p>Compared to CNNs for text classification, language models have several differences.
            Here we discuss general design principles of CNN language models;
            for a detailed description of specific architectures, you can look in the
            <a href="../language_modeling.html#related_papers" target="_blank">Related Papers</a> section
            in the <a href="../language_modeling.html" target="_blank">Language Modeling</a> lecture.
        </p>

        <img src="../../resources/lectures/lang_models/neural/cnn/cnn_main-min.png"
             style="max-width:100%; margin-bottom:20px; "/>

        <p>When designing a CNN language model, you have to keep in mind the following things:</p>
        <ul>
            <li><font face="arial">prevent information flow from future tokens</font><br>
                To predict a token, a left-to-right LM has to use only previous tokens - make sure
                your CNN does not see anything but them! For example, you can shift tokens to the right by using padding - look
                at the illustration above.
            </li>
            <li><font face="arial">do not remove positional information</font><br>
                Differently from text classification, positional information is <font face="arial">very</font>
                important for language models. Therefore, <font face="arial">do not use pooling</font>
                (or be very careful in how you do it).
            </li>
            <li><font face="arial">if you stack many layers, do not forget about residual connections</font><br>
                If you stack many layers, it may difficult to train a very deep network well. To avoid this,
                use residual connections - look for the details below.
            </li>
        </ul>


                <h3 id="receptive_field"><font face="arial">Receptive field</font>: with many layers, can be large</h3>

                <img src="../../resources/lectures/lang_models/neural/cnn/receptive_field-min.png"
             style="max-width:45%; margin-left:20px;float:right;"/>
            <p>When using convolutional models without global pooling,
                    your model will inevitably have a fixed-sized context. This might seem undesirable:
                    the fixed context size problem is exactly what we didn't like in the n-gram models!
                </p>
                <p>However, if for n-gram models typical context size is 1-4,
                    contexts in convolutional models can be quite long. Look at the illustration:
                    with only 3 convolutional layers with small kernel size 3, a network has
                    a context of 7 tokens. If you stack many layers, you can get a very large context length.
                </p>

    <h3><font face="arial">Residual Connections</font>: with many layers, you will need them!</h3>

    <div style="display:grid;grid-template-columns: 40% 60%;">
            <div>
                <p> If you stack many layers, you may have troubles with training a deep network.
                Luckily, for this you can use residual connections!
                </p>
                <p> Look at the example of a convolutional network with residual connections.
                    Typically, we put residual connections around blocks with several layers.
                    A network can several such blocks
                     - remember, you need a lot of layers to get a decent receptive field.
                </p>
            </div>
            <div><img src="../../resources/lectures/lang_models/neural/cnn/cnn_with_residual-min.png"
             style="max-width:90%; margin-left:20px;float:right;"/></div>

        </div>


</div>


    <br>

    <!--
<div id="machine_translation">
    <h1>Specific Tasks: Machine Translation</h1>

        <br>

    <div style="border: 0px solid #ccc;border-radius:15px;margin: 10px; padding: 4px;
 background-color: #f5f5f5;margin-top:-20px;">

            <div style="display: grid; grid-template-columns: 75% 25%; margin:10px;">
                <div>
                <div style="margin-top:20px;">
                    <p style="margin:30px;font-size:30px;">Coming soon!</p>

                    <p style="margin:30px;">This part will appear when we pass the
                        <a href="../../nlp_course.html#preview_seq2seq_attn">Seq2Seq and Attention</a> lecture of the course.</p>
                </div>
                    </div>
                <div>
                    <center>
                    <img src="../../resources/lectures/main/preview/pusheen_reads_on_white-min.png"
                       style="width:80%; padding-top:20px; padding-bottom:20px;border-radius:50%">
                    </center>
                </div>
            </div>

        </div>


</div>
-->


<div id="analysis_interpretability">
        <img height="40" src="../../resources/lectures/ico/analysis_empty.png"
             style="float:left; padding-right:20px; "/>
        <h1>Analysis and Interpretability</h1>

    <h2>What do Convolutions Learn? Analyzing Convolutional Filters</h2>
    <h3><u>Convolutions in Computer Vision: Visual Patterns</u></h3>
    <p>Convolutions were originally developed for images, and there's already a pretty good understanding
    of what the filters capture and how filters from different layers from a hierarchy.
    While lower layers capture simple visual patterns such as lines or circles, final
        layers can capture the whole pictures, animals, people, etc.
    </p>

    <p style="text-align: center;  display: block; margin-left:25px;
         margin-top:-10px; max-width:100%;">
            <img width=90% src="../../resources/lectures/text_clf/analysis/filters_in_images-min.png" alt="" /><br />
            <span style="font-size: small;">
                Examples of patterns captured by convolution filters for images.
                The examples are from
        <a href="https://distill.pub/2019/activation-atlas/" target="_blank">
            Activation Atlas from distill.pub</a>.</span></p>

    <h2>Convolutions for Text Classification</h2>

    <p class="data_text"><font color="#888">This part is from the
        <a href="../text_classification.html" target="_blank">Text Classification</a> lecture from the main part of the course.
    </font>
    </p>


    <p>For images, filters capture local visual patterns which are important for classification.
        For text, such local patterns are word n-grams.
        The main findings on how CNNs work for texts are:</p>
    <ul>
        <li><font face="arial">convolving filters are used as ngram detectors</font><br>
            Each filter specializes in one or several families of closely-related ngrams.
            Filters are not homogeneous, i.e. a single filter can, and often does,
            detect multiple distinctly different families of ngrams.
        </li>
        <li><font face="arial">max-pooling induces a thresholding behavior</font><br>
            Values below a given threshold are ignored when (i.e. irrelevant to) making a prediction.
            For example, <a href="https://arxiv.org/pdf/1809.08037.pdf" target="_blank">this paper</a>
            shows that 40% of the pooled ngrams on average can be dropped with no loss of performance.
        </li>
    </ul>

    <img width=55% src="../../resources/lectures/text_clf/analysis/look_at_filter-min.png" alt=""
         style="float:right; margin-left:20px;" />
    <p>The simplest way to understand what a network captures is to look which patterns activate its neurons.
        For convolutions, we pick a filter and find those n-grams which activate this filter most.
    </p>

    <p>Below are examples of the top-1 n-gram for several filters. For one of them, we also
    show other n-grams which lead to high activation of this filter - you can see that the n-grams
    have a very similar meaning.</p>


    <center>
    <img width=75% src="../../resources/lectures/text_clf/analysis/top_ngrams-min.png" alt=""
    style="margin-bottom:20px;"/>
        </center>

    <p>For more details, look at the paper
        <a href="https://arxiv.org/pdf/1809.08037.pdf" target="_blank">
            Understanding Convolutional Neural Networks for Text Classification</a>.
    </p>


    <h2>Convolutions for Language Modeling</h2>

<p class="data_text"><font color="#888">This part is from the
    <a href="../language_modeling.html#research_thinking" target="_blank">Research Thinking</a> section of
    the <a href="../language_modeling.html" target="_blank">Language Modeling</a>
    lecture from the main part of the course.
    </font>
    </p>

    <p>Let's look at the examples from the EMNLP 2016 paper
    <a href="https://www.aclweb.org/anthology/D16-1123.pdf" target="_blank">Convolutional Neural Network Language Models</a>.
        For a simple convolutional LM, the authors feed the development data to a model and find
                        ngrams that activate a certain filter most.
                    </p>
                    <img width=100% src="../../resources/lectures/lang_models/research/learned_cnn_patterns-min.png"
                           alt="" style="margin-top:20px;margin-bottom:20px;" class="center"/>
                    <p>While a model for sentiment classification learned to pick things which are related to
                    sentiment, the LM model captures phrases which can be continued similarly.
                    For example, one kernel activates on phrases ending with a month, another - with a
                    name; note also the "comparative" kernel firing at
                        <font class="data_text"><strong>as ... as</strong></font>.
                    </p>


    <!--
    <div style="border: 0px solid #ccc;border-radius:15px;margin: 10px; padding: 4px;
 background-color: #f5f5f5;margin-top:-20px;">
    <div style="display: grid; grid-template-columns: 75% 25%; margin:10px;">
                <div>
                <div style="margin-top:20px;">
                    <p style="margin:30px; font-size:30px;">Coming soon!</p>

                    <p style="margin:30px;">This part will appear when we pass the
                        <a href="../../nlp_course.html#preview_lang_models">Language Modeling</a> lecture of the course.</p>
                </div>
                    </div>
                <div>
                    <center>
                    <img src="../../resources/lectures/main/preview/pusheen_draws_on_white-min.png"
                       style="width:80%; padding-top:20px; padding-bottom:20px;border-radius:50%">
                    </center>
                </div>
            </div>

        </div>

        -->
</div>


</div>


</div>
</div>



    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <table >
          <tr class="contact-list">


            <td>
              <div class="media">
                <a class="pull-left thumbnail" href="https://linkedin.com/">
                  <img src="/img/ico/linkedin.png" height=20px alt=""/>
                </a>
              </div>
            </td>

            <td>
              <div class="media">
                <a class="pull-left thumbnail" href="https://github.com/mariagabv">
                  <img src="/img/ico/git.png" height=20px alt=""/>
                </a>
              </div>
            </td>


          </tr>
        </table>
      </div>

      <!--<div class="footer-col  footer-col-2">
      </div> -->

      <div class="footer-col  footer-col-3">
          <p class="text" align="right">Last updated September 03, 2024.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>