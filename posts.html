---
layout: default
title: Blog
description: Intuitive explanations for some of my papers.
menu: yes
order: 1
---

<style>

  #thumbnail {
    box-shadow: 0 5px 10px rgba(0,0,0,0.19), 0 3px 3px rgba(0,0,0,0.23);
  }
  #thumbnail:hover {
    box-shadow: 0 12px 24px rgba(0,0,0,0.19), 0 8px 8px rgba(0,0,0,0.23);
  }

  .fullCard {
    width: 750px;
    border: 1px solid #ccc;
    border-radius: 5px;
    margin: 10px 5px;
    padding: 4px;

  }
  .cardContent {
    padding: 10px;

  }

  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

</style>


<div class="fullCard" id="thumbnail" >
    <div class="cardContent">
      <div style="display: grid; grid-template-columns: 50% 50%; margin:10px;" id="coming_soon_main_course">
        <div style="margin-top:90px;margin-left:40px;">
            <font style="font-size:30px;">Coming soon!</font>
        </div>
        <div>
            <center>
            <img src="../resources/lectures/main/preview/pusheen_draws_on_white-min.png"
               style="width:50%; padding-top:50px;padding-bottom:50px;border-radius:50%">
            </center>
        </div>
    </div>

        <!--<h1 style="font-size:28px;">Language Modeling, Lexical Translation, Reordering</h1>-->
         

    </div>
</div>


<!-- ###################################################################################


<div class="fullCard" id="thumbnail" >
    <div class="cardContent">
        <h1 style="font-size:28px;">The Story of Heads</h1>


<a class="float-right">
    <img src="../img/paper/acl19_heads-min.png" alt="" style="max-width:350px; height:auto; float: right"/>
</a>

<span style="font-size:14px;">
This is a post for the ACL 2019 paper
    <a href="https://www.aclweb.org/anthology/P19-1580" target="_blank">
            Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.
    </a>
</span>

<br/>
<br/>
<span style="font-size:15px;">
From this post, you will learn:
<ul>
  <li>how we evaluate the importance of attention heads in Transformer</li>
  <li>which functions the most important encoder heads perform</li>
  <li>how we prune the vast majority of attention heads in Transformer without seriously affecting quality</li>
    <li>which types of model attention are most sensitive to the number of attention heads and on which layers </li>
</ul>
</span>


<a class="pull-right" href="/posts/acl19_heads.html" onMouseOver="document.readmore.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore" width=120px class="pull-right"></a>
<a class="pull-right" href="https://www.aclweb.org/anthology/P19-1580" target="_blank" onMouseOver="document.readpaper.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper" width=120px class="pull-right"></a>
<a class="pull-right" href="https://github.com/lena-voita/the-story-of-heads" target="_blank" onMouseOver="document.viewcode.src='../resources/posts/buttons/button_view_code_push-min.png';" onMouseOut="document.viewcode.src='../resources/posts/buttons/button_view_code-min.png';">
<img src="../resources/posts/buttons/button_view_code-min.png" name="viewcode" width=120px></a>
<span style="font-size:15px; text-align: right; float: right; color:gray">June 2019</span>

    </div>
</div>-->
